# SoftMax的原理和原因

> 回归问题如何转换为分类问题，有连续的数值转换为离散值，此外输出的单元从一个变成了多个，需要我们引入softmax运算使得输出更加适合离散值的训练和预测

* 显然一个回归值，并且根据回归值的区间划分类别会影响分类的质量（因为这种行为把类别间的距离进行了不合理的设置，同时回归曲线的设置也很trick）

* 一种直观的想法是做多个分类输出，（个人认为可以看做是多个1 vs N的分类组合），输出的形式为对应到每个类别的概率大小，选取“概率值”最大的单元作为类别的输出，那直接使用输出层来进行比较会有什么问题呢？

  * 一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果o1=o3=103o_1=o_3=10^3*o*1=*o*3=103，那么输出值10却又表示图像类别为猫的概率很低。即：直接使用数值不能体现比例的大小

  * 另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量，标签肯定都是1的形式，那么我们也想要最后的输出层也是接近1的形式,这样输出层具有一定的概率分布的意义

  * 所以我们引入softmax:
    $$
    \hat{y_1},\hat{y_2},\hat{y_3} = softmax(o_1,o_2,o_3)
    \\
    \hat{y_1} = \frac{\exp(o_1)}{\sum_{i=1}^3\exp(o_1)},
    \hat{y_2} = \frac{\exp(o_2)}{\sum_{i=1}^3\exp(o_2)},
    \hat{y_3} = \frac{\exp(o_3)}{\sum_{i=1}^3\exp(o_3)}
    $$
    

* 损失函数如何定义

  * 一种思路是mse，但是我们可能并不需要输出完全处于极端的情况

  * 引入交叉熵，可以看到当y=1的时候我们希望y_hat也等于1
    $$
    H(y^{(i)},\hat{y}^{i}) = -\sum_{j=1}^{q}y_j^{i}\log\hat{y}^{i}
    $$

  * 假设训练数据集合的样本数为n，交叉熵损失函数定义为，多次交叉熵损失的均值即可

* 小结

  * softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布
  * softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数
  * 交叉熵适合衡量两个概率分布的差异